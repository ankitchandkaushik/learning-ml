{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGXJDZDX7wxAxG6SSA0Zmx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Need of activation functions**\n","\n","\n","Need of Activation Functions in Machine Learning\n","1. Activation Functions Introduce Non-Linearity\n","\n","Without activation functions, a neural network is just a stack of linear transformations:\n","\n","y = W2 * (W1 * x + b1) + b2\n","\n","\n","No matter how many layers you add, the result is still linear, which means the network can only model linear relationships.\n","\n","Activation functions let the network learn complex, non-linear patterns. For example, image recognition and natural language processing are highly non-linear problems.\n","\n","2. Activation Decides Neuron “Firing”\n","\n","Biologically inspired, a neuron either “fires” or not. Similarly, an activation function determines:\n","\n","If a neuron should activate (pass information forward)\n","\n","How strongly it activates\n","\n","Examples:\n","\n","ReLU outputs 0 for negative inputs → “don’t fire”\n","\n","Positive inputs → pass the signal forward\n","\n","3. Enables Gradient-Based Learning\n","\n","Activation functions like sigmoid, tanh, ReLU are differentiable.\n","\n","Neural networks use backpropagation, which relies on gradients to update weights:\n","\n","W := W - η * (∂L / ∂W)\n","\n","\n","If the activation function isn’t differentiable, we cannot propagate gradients and train the network effectively.\n","\n","4. Controls Output Range\n","\n","Some activation functions bound the output:\n","\n","Sigmoid → [0, 1] (useful for probabilities)\n","\n","Tanh → [-1, 1] (centered around 0, often helps optimization)\n","\n","Softmax → [0, 1], sum = 1 (used for multi-class classification)\n","\n","This is crucial for interpreting outputs and stabilizing training.\n","\n","5. Helps Avoid Vanishing/Exploding Gradients\n","\n","Functions like ReLU, Leaky ReLU, GELU help prevent gradients from becoming too small (vanishing) or too large (exploding) in deep networks.\n","\n","Choosing the right activation function improves training stability.\n","\n","Summary\n","\n","Without activation functions → network is just linear → cannot learn complex patterns.\n","\n","With activation functions → network can approximate any continuous function (universal approximation theorem).\n","\n","They also provide gradient flow, output scaling, and control neuron firing.\n","\n","Analogy\n","\n","Think of a neuron as a light bulb. Activation functions are the switch. Linear networks are like bulbs always dimming proportionally — everything looks the same. Activation functions allow the network to turn bulbs on/off or vary brightness non-linearly, creating rich patterns of light."],"metadata":{"id":"mk7nvGhBGanX"}},{"cell_type":"markdown","source":["Sigmoid\n"],"metadata":{"id":"-2b1iY-oDtMj"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G0CJk5TjDptJ","executionInfo":{"status":"ok","timestamp":1759025452796,"user_tz":-330,"elapsed":49,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}},"outputId":"40a124c7-561f-4464-f7a4-678fc270b286"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sigmoid: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n","Sigmoid derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n"]}],"source":["def sigmoid(x):\n","    \"\"\"\n","    Calculates the sigmoid of the input x.\n","\n","    Args:\n","        x: A scalar or a NumPy array.\n","\n","    Returns:\n","        The sigmoid of x, a value between 0 and 1.\n","    \"\"\"\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    \"\"\"\n","    Calculates the derivative of the sigmoid function.\n","\n","    Args:\n","        x: A scalar or a NumPy array.\n","\n","    Returns:\n","        Derivative of sigmoid at x.\n","    \"\"\"\n","    s = sigmoid(x)\n","    return s * (1 - s)\n","\n","# Example usage:\n","x = np.array([-2, -1, 0, 1, 2])\n","print(\"Sigmoid:\", sigmoid(x))\n","print(\"Sigmoid derivative:\", sigmoid_derivative(x))\n"]},{"cell_type":"code","source":["def tanh(x):\n","    \"\"\"\n","    Calculates the hyperbolic tangent (tanh) of the input x.\n","    \"\"\"\n","    return np.tanh(x)\n","\n","def tanh_derivative(x):\n","    \"\"\"\n","    Derivative of the tanh function.\n","\n","    Returns values in range [0, 1].\n","    \"\"\"\n","    return 1 - np.tanh(x)**2\n","\n","# Example usage:\n","x = np.array([-2, -1, 0, 1, 2])\n","print(\"Tanh:\", tanh(x))\n","print(\"Tanh derivative:\", tanh_derivative(x))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X39jQ_21EA0w","executionInfo":{"status":"ok","timestamp":1759025463530,"user_tz":-330,"elapsed":27,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}},"outputId":"391974e7-2f4f-4179-c142-867cda518d93"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Tanh: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n","Tanh derivative: [0.07065082 0.41997434 1.         0.41997434 0.07065082]\n"]}]},{"cell_type":"code","source":["def relu(x):\n","    \"\"\"\n","    Rectified Linear Unit activation.\n","    \"\"\"\n","    return np.maximum(0, x)\n","\n","def relu_derivative(x):\n","    \"\"\"\n","    Derivative of ReLU: 1 for positive values, 0 for negative.\n","    \"\"\"\n","    return np.where(x > 0, 1, 0)\n","\n","# Example usage:\n","x = np.array([-2, -1, 0, 1, 2])\n","print(\"ReLU:\", relu(x))\n","print(\"ReLU derivative:\", relu_derivative(x))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JLCI7kkFFgz","executionInfo":{"status":"ok","timestamp":1759025489150,"user_tz":-330,"elapsed":24,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}},"outputId":"a228d82a-c265-499d-c9b5-17d67ed69f4a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["ReLU: [0 0 0 1 2]\n","ReLU derivative: [0 0 0 1 1]\n"]}]},{"cell_type":"code","source":["def softplus(x):\n","    \"\"\"\n","    Softplus activation: smooth version of ReLU.\n","    \"\"\"\n","    return np.log(1 + np.exp(x))\n","\n","def softplus_derivative(x):\n","    \"\"\"\n","    Derivative of Softplus is sigmoid.\n","    \"\"\"\n","    return sigmoid(x)\n","\n","# Example usage:\n","x = np.array([-2, -1, 0, 1, 2])\n","print(\"Softplus:\", softplus(x))\n","print(\"Softplus derivative:\", softplus_derivative(x))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btuUvt4MFOyB","executionInfo":{"status":"ok","timestamp":1759025509018,"user_tz":-330,"elapsed":48,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}},"outputId":"5d078216-be45-4a16-84c2-a108e40aa444"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Softplus: [0.12692801 0.31326169 0.69314718 1.31326169 2.12692801]\n","Softplus derivative: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"QQHDD_5JFdip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vbqDwO4oGgKN"},"execution_count":null,"outputs":[]}]}