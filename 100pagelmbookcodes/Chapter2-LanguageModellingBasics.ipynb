{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdmzRFn5a3pgsfazdS97m9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"lTlt2yhQv4C6","executionInfo":{"status":"ok","timestamp":1757147560693,"user_tz":-330,"elapsed":7770,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"outputs":[],"source":["import re, torch, torch.nn as nn"]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","docs = [\n","    \"Movies are fun for everyone.\",\n","    \"Watching movies is great fun. \",\n","    \"Enjoy a great movie today. \",\n","    \"Research is interesting and important. \",\n","    \"Learning math is very important. \",\n","    \"Science discovery is interesting. \",\n","    \"Rock is great to listen to. \",\n","    \"Listen to music for fun. \",\n","    \"Music is fun for everyone. \",\n","    \"Listen to folk music! \"\n","\n","]\n","labels = [1, 1, 1, 3, 3, 3, 2, 2, 2, 2]\n","\n","num_classes = len(set(labels))"],"metadata":{"id":"2jDuz1cYwRIG","executionInfo":{"status":"ok","timestamp":1757147560738,"user_tz":-330,"elapsed":41,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def tokenize(text):\n","  return re.findall(r\"\\w+\", text.lower())\n","\n","def get_vocabulary(texts):\n","  tokens = {token for text in texts for token in tokenize(text)}\n","  return {word: idx for idx, word in enumerate(sorted(tokens))}\n","\n","vocabulary = get_vocabulary(docs)"],"metadata":{"id":"fNJpt9nIwo2V","executionInfo":{"status":"ok","timestamp":1757147560739,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def doc_to_bow(doc, vocabulary):\n","  tokens = set(tokenize(doc))\n","  bow = [0]*len(vocabulary)\n","  for token in tokens:\n","    if token in vocabulary:\n","      bow[vocabulary[token]] = 1\n","  return bow\n","\n"],"metadata":{"id":"-eEmhnMlycy8","executionInfo":{"status":"ok","timestamp":1757147560756,"user_tz":-330,"elapsed":17,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["vectors = torch.tensor(\n","    [doc_to_bow(doc, vocabulary) for doc in docs],\n","    dtype=torch.float32\n",")\n","labels = torch.tensor(labels, dtype=torch.long) - 1"],"metadata":{"id":"dOxnd0QHzaNc","executionInfo":{"status":"ok","timestamp":1757147560774,"user_tz":-330,"elapsed":17,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["input_dim = len(vocabulary)\n","hidden_dim = 50\n","output_dim = num_classes\n","\n","class SimpleClassifier(nn.Module):\n","  def __init__(self, input_dim, hidden_dim, output_dim):\n","    super().__init__()\n","    self.fc1 = nn.Linear(input_dim, hidden_dim)\n","    self.relu = nn.ReLU()\n","    self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","    x = self.relu(x)\n","    x = self.fc2(x)\n","    return x\n","\n","model = SimpleClassifier(input_dim, hidden_dim, output_dim)"],"metadata":{"id":"QIOpUwpNzvxz","executionInfo":{"status":"ok","timestamp":1757147560798,"user_tz":-330,"elapsed":22,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n","\n","for step in range(3000):\n","  optimizer.zero_grad()\n","  loss = criterion(model(vectors), labels)\n","  loss.backward()\n","  optimizer.step()"],"metadata":{"id":"FNAfYH1N0d6f","executionInfo":{"status":"ok","timestamp":1757147571790,"user_tz":-330,"elapsed":10982,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["new_docs = [\n","\"Listening to rock music is fun.\",\n","\"I love science very much.\"\n","]\n","class_names = [\"Cinema\", \"Music\", \"Science\"]\n","\n","new_doc_vectors = torch.tensor(\n","    [doc_to_bow(new_docs, vocabulary) for new_docs in new_docs],\n","    dtype=torch.float32\n",")\n","\n","\n","with torch.no_grad():\n","  outputs = model(new_doc_vectors)\n","  predicted_ids = torch.argmax(outputs, dim=1)+1\n","  predicted_classes = [class_names[id-1] for id in predicted_ids]\n","\n","for i, new_doc in enumerate(new_docs):\n","  print(f\"New document: {new_doc}\")\n","  print(f\"Predicted class: {predicted_classes[i]}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wsAbiqj1KCi","executionInfo":{"status":"ok","timestamp":1757147571819,"user_tz":-330,"elapsed":20,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}},"outputId":"04c72381-21b0-48f1-b5f2-cb380eb34dff"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["New document: Listening to rock music is fun.\n","Predicted class: Music\n","\n","New document: I love science very much.\n","Predicted class: Science\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CxmdjHyl2Ivc","executionInfo":{"status":"ok","timestamp":1757147571834,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#Byte-Pair Encoding\n"],"metadata":{"id":"f0Eu0pjMx6SS"}},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def initialize_vocabulary(corpus):\n","  vocabulary = defaultdict(int)\n","  charset = set()\n","  for word in corpus:\n","    word_with_marker = '_' + word\n","    characters = list(word_with_marker)\n","    charset.update(characters)\n","    tokenized_word = ' '.join(characters)\n","    vocabulary[tokenized_word] += 1\n","  return vocabulary, charset\n","\n","\n","def get_pair_counts(vocabulary):\n","  pair_counts = defaultdict(int)\n","  for tokenized_word, count in vocabulary.items():\n","    tokens = tokenized_word.split()\n","    for i in range(len(tokens) - 1):\n","      pair = (tokens[i], tokens[i+1])\n","      pair_counts[pair] += count\n","  return pair_counts\n","\n","\n","def merge_pair(vocabulary, pair):\n","  new_vocabulary = {}\n","  bigram = re.escape(' '.join(pair))\n","  pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n","  for tokenized_word, count in vocabulary.items():\n","    new_tokenized_word = pattern.sub(''.join(pair), tokenized_word)\n","    new_vocabulary[new_tokenized_word] = count\n","\n","\n","def byte_pair_encoding(corpus, vocab_size):\n","  vocabulary, charset = initialize_vocabulary(corpus)\n","  merges = []\n","  tokens = set(charset)\n","  while len(tokens) < vocab_size:\n","    pair_counts = get_pair_counts(vocabulary)\n","    if not pair_counts:\n","      break\n","    most_frequent_pair = max(pair_counts, key=pair_counts.get)\n","    merges.append(most_frequent_pair)\n","    vocabulary = merge_pair(vocabulary, most_frequent_pair)\n","    tokens.add(''.join(most_frequent_pair))\n","  return vocabulary, merges, charset, tokens"],"metadata":{"id":"ZWTsntY7x9uM","executionInfo":{"status":"ok","timestamp":1757147571865,"user_tz":-330,"elapsed":19,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# function to tokenize using trained tokenizer\n","def tokenize_word(word, merges, vocabulary, charset, unk_token=\"<UNK>\"):\n","  word = \"_\" + word\n","  tokens = [char if char in charset else unk_token for char in word]\n","  for left, right in merges:\n","    i=0\n","    while i < len(tokens) - 1:\n","      if tokens[i:i+2] == [left, right]:\n","        tokens[i:i+2] = [left+right]\n","      else:\n","        i+=1\n","\n","  return tokens\n"],"metadata":{"id":"290RZRmd41eJ","executionInfo":{"status":"ok","timestamp":1757147571888,"user_tz":-330,"elapsed":1,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uUJahH2B6dCY","executionInfo":{"status":"ok","timestamp":1757147572078,"user_tz":-330,"elapsed":189,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["#Language Model"],"metadata":{"id":"15pjzGTDHTx9"}},{"cell_type":"code","source":["# Count based -  n-gram\n","\n","class CountLanguageModel:\n","  def __init__(self, n):\n","    self.n = n\n","    self.ngram_counts = [{} for _ in range(n)]\n","    self.total_unigrams = 0\n","\n","\n","  def predict_next_token(self, context):\n","    for n in range(self.n, 1, -1):\n","      if(len(context) >= n-1):\n","        context_n = tuple(context[-(n-1):])\n","        counts = self.ngram_counts[n-1].get(context_n)\n","        if counts:\n","          return max(counts.items(), key=lambda x: x[1])[0]\n","\n","    unigram_count = self.ngram_counts[0].get(())\n","    if unigram_count:\n","      return max(unigram_count.items(), key=lambda x: x[1])[0]\n","    return None\n","\n","\n","  def get_probability(self, token, context):\n","    for n in range(self.n, 1, -1):\n","        if len(context) >= n - 1:\n","            context_n = tuple(context[-(n - 1):])\n","            counts = self.ngram_counts[n - 1].get(context_n)\n","            if counts:\n","                total = sum(counts.values())\n","                count = counts.get(token, 0)\n","                if count > 0:\n","                    return count / total\n","    unigram_counts = self.ngram_counts[0].get(())\n","    count = unigram_counts.get(token, 0)\n","    V = len(unigram_counts)\n","    return (count + 1) / (self.total_unigrams + V)"],"metadata":{"id":"9OR8uUNrHTmP","executionInfo":{"status":"ok","timestamp":1757149009136,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["def train_model(model, tokens):\n","  model.total_unigrams = len(tokens)\n","  for n in range(1, model.n + 1):\n","    counts = model.ngram_counts[n-1]\n","    for i in range(len(tokens) - n + 1):\n","      context = tuple(tokens[i:i+n-1])\n","      next_token = tokens[i+n-1]\n","      if context not in counts:\n","        counts[context] = defaultdict(int)\n","      counts[context][next_token] = counts[context][next_token] + 1"],"metadata":{"id":"hFHmmAYaHTfk","executionInfo":{"status":"ok","timestamp":1757149009197,"user_tz":-330,"elapsed":7,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["import requests   # For downloading the corpus\n","import gzip       # For decompressing the downloaded corpus\n","import io         # For handling byte streams\n","import math       # For mathematical operations (log, exp)\n","import random     # For random number generation\n","from collections import defaultdict  # For efficient dictionary operations\n","import pickle, os # For saving and loading the model\n","\n","\n","def set_seed(seed):\n","    \"\"\"\n","    Sets random seeds for reproducibility.\n","\n","    Args:\n","        seed (int): Seed value for the random number generator\n","    \"\"\"\n","    random.seed(seed)\n","\n","def download_corpus(url):\n","    \"\"\"\n","    Downloads and decompresses a gzipped corpus file from the given URL.\n","\n","    Args:\n","        url (str): URL of the gzipped corpus file\n","\n","    Returns:\n","        str: Decoded text content of the corpus\n","\n","    Raises:\n","        HTTPError: If the download fails\n","    \"\"\"\n","    print(f\"Downloading corpus from {url}...\")\n","    response = requests.get(url)\n","    response.raise_for_status()  # Raises an exception for bad HTTP responses\n","\n","    print(\"Decompressing and reading the corpus...\")\n","    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n","        corpus = f.read().decode('utf-8')\n","\n","    print(f\"Corpus size: {len(corpus)} characters\")\n","    return corpus\n","\n","def generate_text(model, context, num_tokens):\n","    \"\"\"\n","    Generates text by repeatedly sampling from the model.\n","\n","    Args:\n","        model (CountLanguageModel): Trained language model\n","        context (list): Initial context tokens\n","        num_tokens (int): Number of tokens to generate\n","\n","    Returns:\n","        str: Generated text including initial context\n","    \"\"\"\n","    # Start with the provided context\n","    generated = list(context)\n","\n","    # Generate new tokens until we reach the desired length\n","    while len(generated) - len(context) < num_tokens:\n","        # Use the last n-1 tokens as context for prediction\n","        next_token = model.predict_next_token(generated[-(model.n-1):])\n","        generated.append(next_token)\n","\n","        # Stop if we've generated enough tokens AND found a period\n","        # This helps ensure complete sentences\n","        if len(generated) - len(context) >= num_tokens and next_token == '.':\n","            break\n","\n","    # Join tokens with spaces to create readable text\n","    return ' '.join(generated)\n","\n","def compute_perplexity(model, tokens, context_size):\n","    \"\"\"\n","    Computes perplexity of the model on given tokens.\n","\n","    Args:\n","        model (CountLanguageModel): Trained language model\n","        tokens (list): List of tokens to evaluate on\n","        context_size (int): Maximum context size to consider\n","\n","    Returns:\n","        float: Perplexity score (lower is better)\n","    \"\"\"\n","    # Handle empty token list\n","    if not tokens:\n","        return float('inf')\n","\n","    # Initialize log likelihood accumulator\n","    total_log_likelihood = 0\n","    num_tokens = len(tokens)\n","\n","    # Calculate probability for each token given its context\n","    for i in range(num_tokens):\n","        # Get appropriate context window, handling start of sequence\n","        context_start = max(0, i - context_size)\n","        context = tuple(tokens[context_start:i])\n","        token = tokens[i]\n","\n","        # Get probability of this token given its context\n","        probability = model.get_probability(token, context)\n","\n","        # Add log probability to total (using log for numerical stability)\n","        total_log_likelihood += math.log(probability)\n","\n","    # Calculate average log likelihood\n","    average_log_likelihood = total_log_likelihood / num_tokens\n","\n","    # Convert to perplexity: exp(-average_log_likelihood)\n","    # Lower perplexity indicates better model performance\n","    perplexity = math.exp(-average_log_likelihood)\n","    return perplexity\n","\n","def tokenize(text):\n","    \"\"\"\n","    Tokenizes text into words and periods.\n","\n","    Args:\n","        text (str): Input text to tokenize\n","\n","    Returns:\n","        list: List of lowercase tokens matching words or periods\n","    \"\"\"\n","    return re.findall(r\"\\b[a-zA-Z0-9]+\\b|[.]\", text.lower())\n","\n","def download_and_prepare_data(data_url):\n","    \"\"\"\n","    Downloads and prepares training and test data.\n","\n","    Args:\n","        data_url (str): URL of the corpus to download\n","\n","    Returns:\n","        tuple: (training_tokens, test_tokens) split 90/10\n","    \"\"\"\n","    # Download and extract the corpus\n","    corpus = download_corpus(data_url)\n","\n","    # Convert text to tokens\n","    tokens = tokenize(corpus)\n","\n","    # Split into training (90%) and test (10%) sets\n","    split_index = int(len(tokens) * 0.9)\n","    train_corpus = tokens[:split_index]\n","    test_corpus = tokens[split_index:]\n","\n","    return train_corpus, test_corpus\n","\n","def save_model(model, model_name):\n","    \"\"\"\n","    Saves the trained language model to disk.\n","\n","    Args:\n","        model (CountLanguageModel): Trained model to save\n","        model_name (str): Name to use for the saved model file\n","\n","    Returns:\n","        str: Path to the saved model file\n","\n","    Raises:\n","        IOError: If there's an error writing to disk\n","    \"\"\"\n","    # Create models directory if it doesn't exist\n","    os.makedirs('models', exist_ok=True)\n","\n","    # Construct file path\n","    model_path = os.path.join('models', f'{model_name}.pkl')\n","\n","    try:\n","        print(f\"Saving model to {model_path}...\")\n","        with open(model_path, 'wb') as f:\n","            pickle.dump({\n","                'n': model.n,\n","                'ngram_counts': model.ngram_counts,\n","                'total_unigrams': model.total_unigrams\n","            }, f)\n","        print(\"Model saved successfully.\")\n","        return model_path\n","    except IOError as e:\n","        print(f\"Error saving model: {e}\")\n","        raise\n","\n","def load_model(model_name):\n","    \"\"\"\n","    Loads a trained language model from disk.\n","\n","    Args:\n","        model_name (str): Name of the model to load\n","\n","    Returns:\n","        CountLanguageModel: Loaded model instance\n","\n","    Raises:\n","        FileNotFoundError: If the model file doesn't exist\n","        IOError: If there's an error reading the file\n","    \"\"\"\n","    model_path = os.path.join('models', f'{model_name}.pkl')\n","\n","    try:\n","        print(f\"Loading model from {model_path}...\")\n","        with open(model_path, 'rb') as f:\n","            model_data = pickle.load(f)\n","\n","        # Create new model instance\n","        model = CountLanguageModel(model_data['n'])\n","\n","        # Restore model state\n","        model.ngram_counts = model_data['ngram_counts']\n","        model.total_unigrams = model_data['total_unigrams']\n","\n","        print(\"Model loaded successfully.\")\n","        return model\n","    except FileNotFoundError:\n","        print(f\"Model file not found: {model_path}\")\n","        raise\n","    except IOError as e:\n","        print(f\"Error loading model: {e}\")\n","        raise\n","\n","def get_hyperparameters():\n","    \"\"\"\n","    Returns model hyperparameters.\n","\n","    Returns:\n","        int: Size of n-grams to use in the model\n","    \"\"\"\n","    n = 5\n","    return n"],"metadata":{"id":"AKFW53YdMCSf","executionInfo":{"status":"ok","timestamp":1757149009688,"user_tz":-330,"elapsed":28,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Main model training block\n","if __name__ == \"__main__\":\n","    # Initialize random seeds for reproducibility\n","    set_seed(42)\n","    n = get_hyperparameters()\n","    model_name = \"count_model\"\n","\n","    # Download and prepare the Brown corpus\n","    data_url = \"https://www.thelmbook.com/data/brown\"\n","    train_corpus, test_corpus = download_and_prepare_data(data_url)\n","\n","    # Train the model and evaluate its performance\n","    print(\"\\nTraining the model...\")\n","    model = CountLanguageModel(n)\n","    train_model(model, train_corpus)\n","    print(\"\\nModel training complete.\")\n","\n","    perplexity = compute_perplexity(model, test_corpus, n)\n","    print(f\"\\nPerplexity on test corpus: {perplexity:.2f}\")\n","\n","    save_model(model, model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUr7z4TiNWhg","executionInfo":{"status":"ok","timestamp":1757149031564,"user_tz":-330,"elapsed":21863,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}},"outputId":"7b2ab099-982d-44a8-ac17-1b18dc3980d6"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading corpus from https://www.thelmbook.com/data/brown...\n","Decompressing and reading the corpus...\n","Corpus size: 6185606 characters\n","\n","Training the model...\n","\n","Model training complete.\n","\n","Perplexity on test corpus: 299.06\n","Saving model to models/count_model.pkl...\n","Model saved successfully.\n"]}]},{"cell_type":"code","source":["\n","# Main model testing block\n","if __name__ == \"__main__\":\n","\n","    model = load_model(model_name)\n","\n","    # Test the model with some example contexts\n","    contexts = [\n","        \"i will build a\",\n","        \"the best place to\",\n","        \"she was riding a\"\n","    ]\n","\n","    # Generate completions for each context\n","    for context in contexts:\n","        tokens = tokenize(context)\n","        next_token = model.predict_next_token(tokens)\n","        print(f\"\\nContext: {context}\")\n","        print(f\"Next token: {next_token}\")\n","        print(f\"Generated text: {generate_text(model, tokens, 10)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4u4KtwPOKFf","executionInfo":{"status":"ok","timestamp":1757149035616,"user_tz":-330,"elapsed":4048,"user":{"displayName":"Ankit Chand","userId":"12795931430352336162"}},"outputId":"7d76cc45-3419-421a-dde1-b7e7f8835e06"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model from models/count_model.pkl...\n","Model loaded successfully.\n","\n","Context: i will build a\n","Next token: wall\n","Generated text: i will build a wall to keep the people in and added so long\n","\n","Context: the best place to\n","Next token: live\n","Generated text: the best place to live in 30 per cent to get happiness for yourself\n","\n","Context: she was riding a\n","Next token: horse\n","Generated text: she was riding a horse and showing a dog are very similar your aids\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z5FFNhc3OxRJ"},"execution_count":null,"outputs":[]}]}