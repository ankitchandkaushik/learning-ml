{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "338bnqQb3uOX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import gzip\n",
        "import requests\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(tokenizer, text, retunr_tensor=False):\n",
        "  if retunr_tensor:\n",
        "    return tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  else:\n",
        "    return tokenizer.encode(text,add_special_tokens=False)\n",
        "\n",
        "def decode_text(tokenizer, token_ids):\n",
        "    return tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "class PromptCompletionDataset(Dataset):\n",
        "\n",
        "  def __init__(self, data, tokenizer ):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.data[idx]\n",
        "    prompt = item[\"prompt\"]\n",
        "    completion = item[\"completion\"]\n",
        "\n",
        "    encoded_prompt = encode_text(self.tokenizer, prompt)\n",
        "    encoded_completion = encode_text(self.tokenizer, completion)\n",
        "    eos_token = self.tokenizer.eos_token_id\n",
        "\n",
        "    input_ids = encoded_prompt + encoded_completion + [eos_token]\n",
        "\n",
        "    labels = [-100] * len(encoded_prompt) + encoded_completion + [eos_token]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": labels,\n",
        "        \"prompt\": prompt,\n",
        "        \"expected_completion\": completion\n",
        "    }\n",
        "\n",
        "def collate_fun(batch):\n",
        "  max_length = max([len(item[\"input_ids\"]) for item in batch])\n",
        "\n",
        "  input_ids = [\n",
        "      item[\"input_ids\"] + [tokenizer.pad_token_id] * (max_length - len(item[\"input_ids\"]))\n",
        "      for item in batch\n",
        "  ]\n",
        "\n",
        "  labels = [\n",
        "      item[\"labels\"] + [-100] * (max_length - len(item[\"labels\"]))\n",
        "      for item in batch\n",
        "  ]\n",
        "\n",
        "  attention_mask = [[1] * len(item[\"input_ids\"]) + [0] * (max_length - len(item[\"input_ids\"])) for item in batch]\n",
        "\n",
        "  prompts = [item[\"prompt\"] for item in batch]\n",
        "  expected_completions = [item[\"expected_completion\"] for item in batch]\n",
        "\n",
        "  return {\n",
        "      \"input_ids\": torch.tensor(input_ids),\n",
        "      \"labels\": torch.tensor(labels),\n",
        "      \"attention_mask\": torch.tensor(attention_mask),\n",
        "      \"prompts\": prompts,\n",
        "      \"expected_completions\": expected_completions\n",
        "  }"
      ],
      "metadata": {
        "id": "XyVhBg3_31hP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hyperparameters():\n",
        "  return 2, 16, 5e-5\n",
        "\n",
        "def build_prompt(text):\n",
        "  return f\"Predict the emotion for the following text: {text}\\nEmotion:\"\n",
        "\n",
        "def download_and_prepare_data(data_url, tokenizer, batch_size, test_ratio=0.1):\n",
        "  response = requests.get(data_url)\n",
        "  content = gzip.decompress(response.content).decode()\n",
        "\n",
        "  dataset = []\n",
        "  for entry in map(json.loads, content.splitlines()):\n",
        "    dataset.append({\n",
        "        \"prompt\": build_prompt(entry['text']),\n",
        "        \"completion\": entry[\"label\"].strip()\n",
        "    })\n",
        "  random.shuffle(dataset)\n",
        "  split_index = int(len(dataset)* (1-test_ratio))\n",
        "  train_data = dataset[:split_index]\n",
        "  test_data = dataset[split_index:]\n",
        "\n",
        "  train_dataset = PromptCompletionDataset(train_data, tokenizer)\n",
        "  test_dataset = PromptCompletionDataset(test_data, tokenizer)\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fun)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fun)\n",
        "\n",
        "  return train_dataloader, test_dataloader\n",
        "\n",
        "\n",
        "#copied\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Sets random seeds for reproducibility across different libraries.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Seed value for random number generation\n",
        "    \"\"\"\n",
        "    # Set Python's built-in random seed\n",
        "    random.seed(seed)\n",
        "    # Set PyTorch's CPU random seed\n",
        "    torch.manual_seed(seed)\n",
        "    # Set seed for all available GPUs\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Request cuDNN to use deterministic algorithms\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Disable cuDNN's auto-tuner for consistent behavior\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalizes text for consistent comparison.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "        str: Normalized text\n",
        "    \"\"\"\n",
        "    # Remove leading/trailing whitespace and convert to lowercase\n",
        "    text = text.strip().lower()\n",
        "    # Replace multiple whitespace characters with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_new_tokens=50):\n",
        "    \"\"\"\n",
        "    Generates text completion for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned model\n",
        "        tokenizer: Associated tokenizer\n",
        "        prompt (str): Input prompt\n",
        "        max_new_tokens (int): Maximum number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        str: Generated completion\n",
        "    \"\"\"\n",
        "    # Encode prompt and move to model's device\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate completion using model\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input_ids[\"input_ids\"],\n",
        "        attention_mask=input_ids[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        use_cache=True,        # Use KV cache for faster generation\n",
        "        num_beams=1,           # Use greedy decoding\n",
        "        do_sample=False,       # Don't use sampling\n",
        "    )[0]\n",
        "\n",
        "    # Extract and decode only the generated part (excluding prompt)\n",
        "    generated_text = decode_text(tokenizer, output_ids[input_ids[\"input_ids\"].shape[1]:])\n",
        "    return generated_text.strip()\n",
        "\n",
        "def calculate_accuracy(model, tokenizer, loader):\n",
        "    \"\"\"\n",
        "    Calculates prediction accuracy on a dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Fine-tuned model\n",
        "        tokenizer: Associated tokenizer\n",
        "        loader: DataLoader containing evaluation examples\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy score\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode (disables dropout, etc.)\n",
        "    model.eval()\n",
        "    # Initialize counters for accuracy calculation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient computation for efficiency\n",
        "    with torch.no_grad():\n",
        "        # Iterate through batches\n",
        "        for input_ids, attention_mask, labels, prompts, expected_completions in loader:\n",
        "            # Process each example in the batch\n",
        "            for prompt, expected_completion in zip(prompts, expected_completions):\n",
        "                # Generate model's prediction for this prompt\n",
        "                generated_text = generate_text(model, tokenizer, prompt)\n",
        "                # Compare normalized versions of prediction and expected completion\n",
        "                if normalize_text(generated_text) == normalize_text(expected_completion):\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    # Calculate accuracy, handling empty dataset case\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    # Reset model to training mode\n",
        "    model.train()\n",
        "    return accuracy\n",
        "\n",
        "def test_model(model_path, test_input):\n",
        "    \"\"\"\n",
        "    Tests a saved model on a single input.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to saved model\n",
        "        test_input (str): Text to classify\n",
        "    \"\"\"\n",
        "    # Determine device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load saved model and move to appropriate device\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # Ensure model has proper padding token configuration\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Create prompt and generate prediction\n",
        "    prompt = build_prompt(test_input)\n",
        "    generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"Generated emotion: {generated_text}\")"
      ],
      "metadata": {
        "id": "DbK_ZW7o31ji"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  set_seed(41)\n",
        "\n",
        "  data_url = \"https://www.thelmbook.com/data/emotions\"\n",
        "  model_name = \"openai-community/gpt2\"\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device: {device}\")\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "  num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
        "\n",
        "  train_loader, test_loader = download_and_prepare_data(data_url, tokenizer, batch_size)\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "      input_ids = batch[\"input_ids\"].to(device)\n",
        "      attention_mask = batch[\"attention_mask\"].to(device)\n",
        "      labels = batch[\"labels\"].to(device)\n",
        "\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      num_batches += 1\n",
        "\n",
        "      progress_bar.set_postfix({\"loss\": total_loss / num_batches})\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    test_acc = calculate_accuracy(model, tokenizer, test_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}, test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "# Calculate final model performance\n",
        "  train_acc = calculate_accuracy(model, tokenizer, train_loader)\n",
        "  print(f\"Training accuracy: {train_acc:.4f}\")\n",
        "  print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "  # Save the trained model and tokenizer\n",
        "  model.save_pretrained(\"./finetuned_model\")\n",
        "  tokenizer.save_pretrained(\"./finetuned_model\")\n",
        "\n",
        "  # Test model with a sample input\n",
        "  test_input = \"I'm so happy to be able to finetune an LLM!\"\n",
        "  generated_completion = generate_text(model, tokenizer, build_prompt(test_input))\n",
        "  print(f\"Prompt: {test_input}\\nGenerated Completion: {generated_completion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDOgrX7H31mJ",
        "outputId": "b7cfff37-0e8d-4dc7-a62e-8790fc8c25eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Epoch 1/2:   0%|          | 0/1125 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "Epoch 1/2: 100%|██████████| 1125/1125 [05:17<00:00,  3.54it/s, loss=0.123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Average Loss: 0.12278269641763634, test accuracy: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1125/1125 [05:16<00:00,  3.56it/s, loss=0.0585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2, Average Loss: 0.05850858378454318, test accuracy: 0.0000\n",
            "Training accuracy: 0.0000\n",
            "Test accuracy: 0.0000\n",
            "Prompt: I'm so happy to be able to finetune an LLM!\n",
            "Generated Completion: joy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# work on accuracy fn"
      ],
      "metadata": {
        "id": "HTlqa-nJFfT-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0VB1eUtGdH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}